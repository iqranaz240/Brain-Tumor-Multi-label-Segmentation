{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10273815,"sourceType":"datasetVersion","datasetId":6356926},{"sourceId":10279383,"sourceType":"datasetVersion","datasetId":6360813},{"sourceId":10294349,"sourceType":"datasetVersion","datasetId":6371254},{"sourceId":10294425,"sourceType":"datasetVersion","datasetId":6371317}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.222590Z","iopub.execute_input":"2024-12-25T10:16:58.222916Z","iopub.status.idle":"2024-12-25T10:16:58.227536Z","shell.execute_reply.started":"2024-12-25T10:16:58.222894Z","shell.execute_reply":"2024-12-25T10:16:58.226598Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"\n\"\"\"ResNet variants\"\"\"\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\nfrom torch.nn.modules.utils import _pair\n\n__all__ = ['ResNet', 'Bottleneck']\n\n_url_format = 'https://s3.us-west-1.wasabisys.com/resnest/torch/{}-{}.pth'\n\n_model_sha256 = {name: checksum for checksum, name in [\n    ]}\n\n\ndef short_hash(name):\n    if name not in _model_sha256:\n        raise ValueError('Pretrained model for {name} is not available.'.format(name=name))\n    return _model_sha256[name][:8]\n\nresnest_model_urls = {name: _url_format.format(name, short_hash(name)) for\n    name in _model_sha256.keys()\n}\n\nclass DropBlock2D(object):\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError\n\nclass SplAtConv2d(Module):\n    \"\"\"Split-Attention Conv2d\n    \"\"\"\n    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n                 dilation=(1, 1), groups=1, bias=True,\n                 radix=2, reduction_factor=4,\n                 rectify=False, rectify_avg=False, norm_layer=None,\n                 dropblock_prob=0.0, **kwargs):\n        super(SplAtConv2d, self).__init__()\n        padding = _pair(padding)\n        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n        self.rectify_avg = rectify_avg\n        inter_channels = max(in_channels*radix//reduction_factor, 32)\n        self.radix = radix\n        self.cardinality = groups\n        self.channels = channels\n        self.dropblock_prob = dropblock_prob\n        if self.rectify:\n            from rfconv import RFConv2d\n            self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                                 groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n        else:\n            self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n                               groups=groups*radix, bias=bias, **kwargs)\n        self.use_bn = norm_layer is not None\n        if self.use_bn:\n            self.bn0 = norm_layer(channels*radix)\n        self.relu = ReLU(inplace=True)\n        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n        if self.use_bn:\n            self.bn1 = norm_layer(inter_channels)\n        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n        if dropblock_prob > 0.0:\n            self.dropblock = DropBlock2D(dropblock_prob, 3)\n        self.rsoftmax = rSoftMax(radix, groups)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_bn:\n            x = self.bn0(x)\n        if self.dropblock_prob > 0.0:\n            x = self.dropblock(x)\n        x = self.relu(x)\n\n        batch, rchannel = x.shape[:2]\n        if self.radix > 1:\n            if torch.__version__ < '1.5':\n                splited = torch.split(x, int(rchannel//self.radix), dim=1)\n            else:\n                splited = torch.split(x, rchannel//self.radix, dim=1)\n            gap = sum(splited)\n        else:\n            gap = x\n        gap = F.adaptive_avg_pool2d(gap, 1)\n        gap = self.fc1(gap)\n\n        if self.use_bn:\n            gap = self.bn1(gap)\n        gap = self.relu(gap)\n\n        atten = self.fc2(gap)\n        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n\n        if self.radix > 1:\n            if torch.__version__ < '1.5':\n                attens = torch.split(atten, int(rchannel//self.radix), dim=1)\n            else:\n                attens = torch.split(atten, rchannel//self.radix, dim=1)\n            out = sum([att*split for (att, split) in zip(attens, splited)])\n        else:\n            out = atten * x\n        return out.contiguous()\n\nclass rSoftMax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super().__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        batch = x.size(0)\n        if self.radix > 1:\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.sigmoid(x)\n        return x\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n\nclass Bottleneck(nn.Module):\n    \"\"\"ResNet Bottleneck\n    \"\"\"\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 radix=1, cardinality=1, bottleneck_width=64,\n                 avd=False, avd_first=False, dilation=1, is_first=False,\n                 rectified_conv=False, rectify_avg=False,\n                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n        super(Bottleneck, self).__init__()\n        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.dropblock_prob = dropblock_prob\n        self.radix = radix\n        self.avd = avd and (stride > 1 or is_first)\n        self.avd_first = avd_first\n\n        if self.avd:\n            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n            stride = 1\n\n        if dropblock_prob > 0.0:\n            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n            if radix == 1:\n                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n\n        if radix >= 1:\n            self.conv2 = SplAtConv2d(\n                group_width, group_width, kernel_size=3,\n                stride=stride, padding=dilation,\n                dilation=dilation, groups=cardinality, bias=False,\n                radix=radix, rectify=rectified_conv,\n                rectify_avg=rectify_avg,\n                norm_layer=norm_layer,\n                dropblock_prob=dropblock_prob)\n        elif rectified_conv:\n            from rfconv import RFConv2d\n            self.conv2 = RFConv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False,\n                average_mode=rectify_avg)\n            self.bn2 = norm_layer(group_width)\n        else:\n            self.conv2 = nn.Conv2d(\n                group_width, group_width, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation,\n                groups=cardinality, bias=False)\n            self.bn2 = norm_layer(group_width)\n\n        self.conv3 = nn.Conv2d(\n            group_width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes*4)\n\n        if last_gamma:\n            from torch.nn.init import zeros_\n            zeros_(self.bn3.weight)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.dilation = dilation\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock1(out)\n        out = self.relu(out)\n\n        if self.avd and self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv2(out)\n        if self.radix == 0:\n            out = self.bn2(out)\n            if self.dropblock_prob > 0.0:\n                out = self.dropblock2(out)\n            out = self.relu(out)\n\n        if self.avd and not self.avd_first:\n            out = self.avd_layer(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.dropblock_prob > 0.0:\n            out = self.dropblock3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass ResNeSt(nn.Module):\n    \"\"\"ResNet Variants\n\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n        for Synchronized Cross-GPU BachNormalization).\n\n    Reference:\n\n        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n    \"\"\"\n    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n                 num_classes=1000, dilated=False, dilation=1,\n                 deep_stem=False, stem_width=64, avg_down=False,\n                 rectified_conv=False, rectify_avg=False,\n                 avd=False, avd_first=False,\n                 final_drop=0.0, dropblock_prob=0,\n                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n        self.cardinality = groups\n        self.bottleneck_width = bottleneck_width\n        # ResNet-D params\n        self.inplanes = stem_width*2 if deep_stem else 64\n        self.avg_down = avg_down\n        self.last_gamma = last_gamma\n        # ResNeSt params\n        self.radix = radix\n        self.avd = avd\n        self.avd_first = avd_first\n\n        super(ResNeSt, self).__init__()\n        self.rectified_conv = rectified_conv\n        self.rectify_avg = rectify_avg\n        if rectified_conv:\n            from rfconv import RFConv2d\n            conv_layer = RFConv2d\n        else:\n            conv_layer = nn.Conv2d\n        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n        if deep_stem:\n            self.conv1 = nn.Sequential(\n                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n                norm_layer(stem_width),\n                nn.ReLU(inplace=True),\n                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n            )\n        else:\n            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n                                   bias=False, **conv_kwargs)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n        if dilated or dilation == 4:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=4, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        elif dilation==2:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           dilation=1, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                           dilation=2, norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           norm_layer=norm_layer,\n                                           dropblock_prob=dropblock_prob)\n        self.avgpool = GlobalAvgPool2d()\n        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, norm_layer):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n                    dropblock_prob=0.0, is_first=True):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            down_layers = []\n            if self.avg_down:\n                if dilation == 1:\n                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n                                                    ceil_mode=True, count_include_pad=False))\n                else:\n                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n                                                    ceil_mode=True, count_include_pad=False))\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=1, bias=False))\n            else:\n                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n                                             kernel_size=1, stride=stride, bias=False))\n            down_layers.append(norm_layer(planes * block.expansion))\n            downsample = nn.Sequential(*down_layers)\n\n        layers = []\n        if dilation == 1 or dilation == 2:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n        else:\n            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                radix=self.radix, cardinality=self.cardinality,\n                                bottleneck_width=self.bottleneck_width,\n                                avd=self.avd, avd_first=self.avd_first,\n                                dilation=dilation, rectified_conv=self.rectified_conv,\n                                rectify_avg=self.rectify_avg,\n                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n                                last_gamma=self.last_gamma))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        if self.drop:\n            x = self.drop(x)\n        x = self.fc(x)\n\n        return x\n\n    def forward_feature(self, x, out_block_stage):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x0 = self.maxpool(x)\n\n        x1 = self.layer1(x0)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        if out_block_stage == 1: return [x], x1\n        elif out_block_stage == 2: return [x1, x], x2\n        elif out_block_stage == 3: return [x2, x1, x], x3\n        elif out_block_stage == 4: return [x3, x2, x1, x], x4\n\n\ndef resnet50(pretrained=False, root='~/.encoding/models', **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls['resnet50'], progress=True, check_hash=True))\n    return model\n\ndef resnet101(pretrained=False, root='~/.encoding/models', **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls['resnet101'], progress=True, check_hash=True))\n    return model\n\ndef resnet152(pretrained=False, root='~/.encoding/models', **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(torch.hub.load_state_dict_from_url(\n            resnest_model_urls['resnet152'], progress=True, check_hash=True))\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.326250Z","iopub.execute_input":"2024-12-25T10:16:58.326505Z","iopub.status.idle":"2024-12-25T10:16:58.370885Z","shell.execute_reply.started":"2024-12-25T10:16:58.326484Z","shell.execute_reply":"2024-12-25T10:16:58.369990Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.utils.model_zoo as model_zoo\n\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',\n    'res2net50_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_v1b_26w_4s-3cf99910.pth',\n    'res2net101_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net101_v1b_26w_4s-0812c246.pth',\n    'resnest50': 'https://github.com/zhanghang1989/ResNeSt/releases/download/weights_step1/528c19ca-resnest50.pth'\n}\n\ndef IS2D_model(args) :\n    return MFMSNet(args.num_classes,\n                   args.scale_branches,\n                   args.frequency_branches,\n                   args.frequency_selection,\n                   args.block_repetition,\n                   args.min_channel,\n                   args.min_resolution,\n                   args.cnn_backbone)\n\ndef load_cnn_backbone_model(backbone_name, pretrained=False, **kwargs):\n    if backbone_name=='resnest50':\n        model = ResNeSt(Bottleneck, [3, 4, 6, 3],\n                        radix=2, groups=1, bottleneck_width=64,\n                        deep_stem=True, stem_width=32, avg_down=True,\n                        avd=True, avd_first=False, **kwargs)\n    else:\n        print(\"Invalid backbone\")\n        sys.exit()\n\n    if pretrained:\n        if backbone_name == 'resnest50':\n            model.load_state_dict(torch.load('/kaggle/input/resnest/resnest50-528c19ca.pth'))\n        else:\n            model.load_state_dict(model_zoo.load_url(model_urls[backbone_name]))\n\n        print(\"Complete loading your pretrained backbone {}\".format(backbone_name))\n    return model\n\ndef model_to_device(args, model):\n    model = model.to(args.device)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.372039Z","iopub.execute_input":"2024-12-25T10:16:58.372261Z","iopub.status.idle":"2024-12-25T10:16:58.387041Z","shell.execute_reply.started":"2024-12-25T10:16:58.372243Z","shell.execute_reply":"2024-12-25T10:16:58.386273Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import math\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef get_freq_indices(method):\n    assert method in ['top1','top2','top4','top8','top16','top32',\n                      'bot1','bot2','bot4','bot8','bot16','bot32',\n                      'low1','low2','low4','low8','low16','low32']\n    num_freq = int(method[3:])\n    if 'top' in method:\n        all_top_indices_x = [0,0,6,0,0,1,1,4,5,1,3,0,0,0,3,2,4,6,3,5,5,2,6,5,5,3,3,4,2,2,6,1]\n        all_top_indices_y = [0,1,0,5,2,0,2,0,0,6,0,4,6,3,5,2,6,3,3,3,5,1,1,2,4,2,1,1,3,0,5,3]\n        mapper_x = all_top_indices_x[:num_freq]\n        mapper_y = all_top_indices_y[:num_freq]\n    elif 'low' in method:\n        all_low_indices_x = [0,0,1,1,0,2,2,1,2,0,3,4,0,1,3,0,1,2,3,4,5,0,1,2,3,4,5,6,1,2,3,4]\n        all_low_indices_y = [0,1,0,1,2,0,1,2,2,3,0,0,4,3,1,5,4,3,2,1,0,6,5,4,3,2,1,0,6,5,4,3]\n        mapper_x = all_low_indices_x[:num_freq]\n        mapper_y = all_low_indices_y[:num_freq]\n    elif 'bot' in method:\n        all_bot_indices_x = [6,1,3,3,2,4,1,2,4,4,5,1,4,6,2,5,6,1,6,2,2,4,3,3,5,5,6,2,5,5,3,6]\n        all_bot_indices_y = [6,4,4,6,6,3,1,4,4,5,6,5,2,2,5,1,4,3,5,0,3,1,1,2,4,2,1,1,5,3,3,3]\n        mapper_x = all_bot_indices_x[:num_freq]\n        mapper_y = all_bot_indices_y[:num_freq]\n    else:\n        raise NotImplementedError\n    return mapper_x, mapper_y\n\nclass MultiFrequencyChannelAttention(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 dct_h, dct_w,\n                 frequency_branches=16,\n                 frequency_selection='top',\n                 reduction=16):\n        super(MultiFrequencyChannelAttention, self).__init__()\n\n        assert frequency_branches in [1, 2, 4, 8, 16, 32]\n        frequency_selection = frequency_selection + str(frequency_branches)\n\n        self.num_freq = frequency_branches\n        self.dct_h = dct_h\n        self.dct_w = dct_w\n\n        mapper_x, mapper_y = get_freq_indices(frequency_selection)\n        self.num_split = len(mapper_x)\n        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x]\n        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n\n        assert len(mapper_x) == len(mapper_y)\n\n        # fixed DCT init\n        for freq_idx in range(frequency_branches):\n            self.register_buffer('dct_weight_{}'.format(freq_idx), self.get_dct_filter(dct_h, dct_w, mapper_x[freq_idx], mapper_y[freq_idx], in_channels))\n\n        self.fc = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, stride=1, padding=0, bias=False))\n\n        self.average_channel_pooling = nn.AdaptiveAvgPool2d(1)\n        self.max_channel_pooling = nn.AdaptiveMaxPool2d(1)\n\n    def forward(self, x):\n        batch_size, C, H, W = x.shape\n\n        x_pooled = x\n\n        if H != self.dct_h or W != self.dct_w:\n            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))\n\n        multi_spectral_feature_avg, multi_spectral_feature_max, multi_spectral_feature_min = 0, 0, 0\n        for name, params in self.state_dict().items():\n            if 'dct_weight' in name:\n                x_pooled_spectral = x_pooled * params\n                multi_spectral_feature_avg += self.average_channel_pooling(x_pooled_spectral)\n                multi_spectral_feature_max += self.max_channel_pooling(x_pooled_spectral)\n                multi_spectral_feature_min += -self.max_channel_pooling(-x_pooled_spectral)\n        multi_spectral_feature_avg = multi_spectral_feature_avg / self.num_freq\n        multi_spectral_feature_max = multi_spectral_feature_max / self.num_freq\n        multi_spectral_feature_min = multi_spectral_feature_min / self.num_freq\n\n\n        multi_spectral_avg_map = self.fc(multi_spectral_feature_avg).view(batch_size, C, 1, 1)\n        multi_spectral_max_map = self.fc(multi_spectral_feature_max).view(batch_size, C, 1, 1)\n        multi_spectral_min_map = self.fc(multi_spectral_feature_min).view(batch_size, C, 1, 1)\n\n        multi_spectral_attention_map = F.sigmoid(multi_spectral_avg_map + multi_spectral_max_map + multi_spectral_min_map)\n\n        return x * multi_spectral_attention_map.expand_as(x)\n\n    def get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, in_channels):\n        dct_filter = torch.zeros(in_channels, tile_size_x, tile_size_y)\n\n        for t_x in range(tile_size_x):\n            for t_y in range(tile_size_y):\n                dct_filter[:, t_x, t_y] = self.build_filter(t_x, mapper_x, tile_size_x) * self.build_filter(t_y, mapper_y, tile_size_y)\n\n        return dct_filter\n\n    def build_filter(self, pos, freq, POS):\n        result = math.cos(math.pi * freq * (pos + 0.5) / POS) / math.sqrt(POS)\n        if freq == 0:\n            return result\n        else:\n            return result * math.sqrt(2)\n\nclass MFMSAttentionBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 scale_branches=2,\n                 frequency_branches=16,\n                 frequency_selection='top',\n                 block_repetition=1,\n                 min_channel=64,\n                 min_resolution=8,\n                 groups=32):\n        super(MFMSAttentionBlock, self).__init__()\n\n        self.scale_branches = scale_branches\n        self.frequency_branches = frequency_branches\n        self.block_repetition = block_repetition\n        self.min_channel = min_channel\n        self.min_resolution = min_resolution\n\n        self.multi_scale_branches = nn.ModuleList([])\n        for scale_idx in range(scale_branches):\n            inter_channel = in_channels // 2**scale_idx\n            if inter_channel < self.min_channel: inter_channel = self.min_channel\n\n            self.multi_scale_branches.append(nn.Sequential(\n                nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1 + scale_idx, dilation=1 + scale_idx, groups=groups, bias=False),\n                nn.BatchNorm2d(in_channels), nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels, inter_channel, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(inter_channel), nn.ReLU(inplace=True)\n            ))\n\n        c2wh = dict([(32, 112), (64, 56), (128, 28), (256, 14), (512, 7)])\n        self.multi_frequency_branches = nn.ModuleList([])\n        self.multi_frequency_branches_conv1 = nn.ModuleList([])\n        self.multi_frequency_branches_conv2 = nn.ModuleList([])\n        self.alpha_list = nn.ParameterList([nn.Parameter(torch.ones(1)) for _ in range(scale_branches)])\n        self.beta_list = nn.ParameterList([nn.Parameter(torch.ones(1)) for _ in range(scale_branches)])\n\n        for scale_idx in range(scale_branches):\n            inter_channel = in_channels // 2**scale_idx\n            if inter_channel < self.min_channel: inter_channel = self.min_channel\n\n            if frequency_branches > 0:\n                self.multi_frequency_branches.append(\n                    nn.Sequential(\n                        MultiFrequencyChannelAttention(inter_channel, c2wh[in_channels], c2wh[in_channels], frequency_branches, frequency_selection)))\n            self.multi_frequency_branches_conv1.append(\n                nn.Sequential(\n                    nn.Conv2d(inter_channel, 1, kernel_size=1, stride=1, padding=0, bias=False),\n                    nn.Sigmoid()))\n            self.multi_frequency_branches_conv2.append(\n                nn.Sequential(\n                    nn.Conv2d(inter_channel, in_channels, kernel_size=3, stride=1, padding=1, bias=False),\n                    nn.BatchNorm2d(in_channels), nn.ReLU(inplace=True)))\n\n    def forward(self, x):\n        feature_aggregation = 0\n        for scale_idx in range(self.scale_branches):\n            feature = F.avg_pool2d(x, kernel_size=2 ** scale_idx, stride=2 ** scale_idx, padding=0) if int(x.shape[2] // 2 ** scale_idx) >= self.min_resolution else x\n            feature = self.multi_scale_branches[scale_idx](feature)\n            if self.frequency_branches > 0:\n                feature = self.multi_frequency_branches[scale_idx](feature)\n            spatial_attention_map = self.multi_frequency_branches_conv1[scale_idx](feature)\n            feature = self.multi_frequency_branches_conv2[scale_idx](feature * (1 - spatial_attention_map) * self.alpha_list[scale_idx] + feature * spatial_attention_map * self.beta_list[scale_idx])\n            feature_aggregation += F.interpolate(feature, size=None, scale_factor=2**scale_idx, mode='bilinear', align_corners=None) if (x.shape[2] != feature.shape[2]) or (x.shape[3] != feature.shape[3]) else feature\n        feature_aggregation /= self.scale_branches\n        feature_aggregation += x\n\n        return feature_aggregation\n\nclass UpsampleBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 skip_connection_channels,\n                 scale_branches=2,\n                 frequency_branches=16,\n                 frequency_selection='top',\n                 block_repetition=1,\n                 min_channel=64,\n                 min_resolution=8):\n        super(UpsampleBlock, self).__init__()\n\n        in_channels = in_channels + skip_connection_channels\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))\n\n        self.attention_layer = MFMSAttentionBlock(out_channels, scale_branches, frequency_branches, frequency_selection, block_repetition, min_channel, min_resolution)\n\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True))\n\n    def forward(self, x, skip_connection=None):\n        x = F.interpolate(x, size=None, scale_factor=2, mode='bilinear', align_corners=None)\n\n        x = torch.cat([x, skip_connection], dim=1)\n        x = self.conv1(x)\n        x = self.attention_layer(x)\n        x = self.conv2(x)\n\n        return x\n\nclass CascadedSubDecoderBinary(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 num_classes,\n                 scale_factor,\n                 interpolation_mode='bilinear'):\n        super(CascadedSubDecoderBinary, self).__init__()\n\n        self.output_map_conv = nn.Conv2d(in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.output_distance_conv = nn.Conv2d(in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n        self.output_boundary_conv = nn.Conv2d(in_channels, num_classes, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0))\n\n        self.upsample = nn.Upsample(scale_factor=scale_factor, mode=interpolation_mode, align_corners=True)\n        self.count = 0\n\n    def forward(self, x):\n        map = self.output_map_conv(x) # B, 1, H, W\n        distance = self.output_distance_conv(x) * torch.sigmoid(map)\n        boundary = self.output_boundary_conv(x) * torch.sigmoid(distance)\n\n        boundary = self.upsample(boundary)\n        distance = self.upsample(distance) + torch.sigmoid(boundary)\n        map = self.upsample(map) + torch.sigmoid(distance)\n\n        return map, distance, boundary\n\nclass MFMSNet(nn.Module):\n    def __init__(self,\n                 num_classes=1,\n                 scale_branches=2,\n                 frequency_branches=16,\n                 frequency_selection='top',\n                 block_repetition=1,\n                 min_channel=64,\n                 min_resolution=8,\n                 cnn_backbone='resnet50'):\n        super(MFMSNet, self).__init__()\n\n\n        self.num_classes = num_classes\n\n        self.feature_encoding = load_cnn_backbone_model(backbone_name=cnn_backbone, pretrained=True)\n        \n        self.in_channels = 2048\n        self.skip_channel_list = [1024, 512, 256, 64]\n        self.decoder_channel_list = [256, 128, 64, 32]\n\n        self.feature_encoding.fc = nn.Identity()\n\n        self.skip_channel_down_list = [64, 64, 64, 64]\n\n        self.skip_connection1 = nn.Sequential(\n            nn.Conv2d(self.skip_channel_list[0], self.skip_channel_down_list[0], kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(self.skip_channel_down_list[0]), nn.ReLU(inplace=True))\n        self.skip_connection2 = nn.Sequential(\n            nn.Conv2d(self.skip_channel_list[1], self.skip_channel_down_list[1], kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(self.skip_channel_down_list[1]), nn.ReLU(inplace=True))\n        self.skip_connection3 = nn.Sequential(\n            nn.Conv2d(self.skip_channel_list[2], self.skip_channel_down_list[2], kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(self.skip_channel_down_list[2]), nn.ReLU(inplace=True))\n        self.skip_connection4 = nn.Sequential(\n            nn.Conv2d(self.skip_channel_list[3], self.skip_channel_down_list[3], kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(self.skip_channel_down_list[3]), nn.ReLU(inplace=True))\n\n        self.decoder_stage1 = UpsampleBlock(self.in_channels, self.decoder_channel_list[0], self.skip_channel_down_list[0], scale_branches, frequency_branches, frequency_selection, block_repetition, min_channel, min_resolution)\n        self.decoder_stage2 = UpsampleBlock(self.decoder_channel_list[0], self.decoder_channel_list[1], self.skip_channel_down_list[1], scale_branches, frequency_branches, frequency_selection, block_repetition, min_channel, min_resolution)\n        self.decoder_stage3 = UpsampleBlock(self.decoder_channel_list[1], self.decoder_channel_list[2], self.skip_channel_down_list[2], scale_branches, frequency_branches, frequency_selection, block_repetition, min_channel, min_resolution)\n        self.decoder_stage4 = UpsampleBlock(self.decoder_channel_list[2], self.decoder_channel_list[3], self.skip_channel_down_list[3], scale_branches, frequency_branches, frequency_selection, block_repetition, min_channel, min_resolution)\n\n        # Sub-Decoder\n        self.sub_decoder_stage1 = CascadedSubDecoderBinary(self.decoder_channel_list[0], num_classes, scale_factor=16)\n        self.sub_decoder_stage2 = CascadedSubDecoderBinary(self.decoder_channel_list[1], num_classes, scale_factor=8)\n        self.sub_decoder_stage3 = CascadedSubDecoderBinary(self.decoder_channel_list[2], num_classes, scale_factor=4)\n        self.sub_decoder_stage4 = CascadedSubDecoderBinary(self.decoder_channel_list[3], num_classes, scale_factor=2)\n\n    def forward(self, x, mode='train'):\n        if x.size()[1] == 1: x = x.repeat(1, 3, 1, 1)\n        _, _, H, W = x.shape\n\n        features, x = self.feature_encoding.forward_feature(x, out_block_stage=4)\n\n        x1 = self.decoder_stage1(x, self.skip_connection1(features[0]))\n        x2 = self.decoder_stage2(x1, self.skip_connection2(features[1]))\n        x3 = self.decoder_stage3(x2, self.skip_connection3(features[2]))\n        x4 = self.decoder_stage4(x3, self.skip_connection4(features[3]))\n        if mode == 'train':\n            map_output1, distance_output1, boundary_output1 = self.sub_decoder_stage1(x1)\n            map_output2, distance_output2, boundary_output2 = self.sub_decoder_stage2(x2)\n            map_output3, distance_output3, boundary_output3 = self.sub_decoder_stage3(x3)\n            map_output4, distance_output4, boundary_output4 = self.sub_decoder_stage4(x4)\n\n\n            return [map_output1, distance_output1, boundary_output1], \\\n                   [map_output2, distance_output2, boundary_output2], \\\n                   [map_output3, distance_output3, boundary_output3], \\\n                   [map_output4, distance_output4, boundary_output4]\n        else:\n            map, _, _ = self.sub_decoder_stage4(x4)\n\n            return map\n\n    def _calculate_criterion(self, y_pred, y_true):\n        if isinstance(y_pred, tuple):\n            y_pred = y_pred[0][0]  \n        loss = self.structure_loss(y_pred, y_true)\n        return loss\n\n    import torch\n    import torch.nn.functional as F\n\n    def structure_loss(self, pred, mask):\n        if not isinstance(pred, torch.Tensor) or not isinstance(mask, torch.Tensor):\n            raise TypeError(\"Both pred and mask should be torch tensors\")\n\n        weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n\n        wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction='none')\n\n        wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n\n        pred_prob = torch.sigmoid(pred)\n\n        inter = ((pred_prob * mask) * weit).sum(dim=(2, 3))\n        union = ((pred_prob + mask) * weit).sum(dim=(2, 3))\n\n        wiou = 1 - (inter + 1) / (union - inter + 1)\n\n        total_loss = (wbce + wiou).mean()  \n\n        return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.388460Z","iopub.execute_input":"2024-12-25T10:16:58.388708Z","iopub.status.idle":"2024-12-25T10:16:58.430685Z","shell.execute_reply.started":"2024-12-25T10:16:58.388689Z","shell.execute_reply":"2024-12-25T10:16:58.430006Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import scipy\nimport numpy as np\nfrom skimage import morphology\nfrom skimage.measure import label, regionprops\nfrom scipy import ndimage\nfrom scipy.ndimage import convolve, distance_transform_edt as bwdist\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, jaccard_score, confusion_matrix)\n\n\nmetrics_list = ['DSC', 'IoU', 'WeightedF-Measure', 'S-Measure', 'E-Measure', 'MAE']\n\nclass BMIS_Metrics_Calculator(object):\n    def __init__(self, metrics_list):\n        super(BMIS_Metrics_Calculator).__init__()\n\n        self.metrics_list = metrics_list\n\n        self.smooth = 1e-5\n\n        self.total_metrics_dict = dict()\n\n        for metric in self.metrics_list:\n            self.total_metrics_dict[metric] = list()\n\n    def get_metrics_dict(self, y_pred, y_true):\n        y_true = y_true.squeeze().detach().cpu().numpy()\n        y_pred = (y_pred.squeeze().detach().cpu().numpy() >= 0.5).astype(np.int_)\n        # y_pred = y_pred.squeeze().detach().cpu().numpy()\n\n        y_true = np.asarray(y_true, np.float32)\n        y_true /= (y_true.max() + 1e-8)\n        y_true[y_true > 0.5] = 1; y_true[y_true != 1] = 0\n\n        metrics_dict = dict()\n\n        for metric in self.metrics_list:\n            metrics_dict[metric] = 0\n            result = self.get_metrics(metric, y_pred, y_true)\n            if np.isnan(result): result = 1e-6\n            metrics_dict[metric] = result\n\n        return metrics_dict\n\n    def get_metrics(self, metric, y_pred, y_true):\n        if metric == 'Accuracy': return self.calculate_Accuracy(y_pred, y_true)\n        elif metric == 'DSC': return self.calculate_DSC(y_pred, y_true)\n        elif metric == 'Precision': return self.calculate_Precision(y_pred, y_true)\n        elif metric == 'Recall': return self.calculate_Recall(y_pred, y_true)\n        elif metric == 'Specificity': return self.calculate_Specificity(y_pred, y_true)\n        elif metric == 'Jaccard': return self.calculate_Jaccard(y_pred, y_true)\n        elif metric == 'IoU': return self.calculate_IoU(y_pred, y_true)\n        elif metric == 'WeightedF-Measure':  return self.calculate_WeightedFMeasure(y_pred, y_true)\n        elif metric == 'F-Measure': return self.calculate_FMeasure(y_pred, y_true)\n        elif metric == 'S-Measure': return self.calculate_SMeasure(y_pred, y_true)\n        elif metric == 'E-Measure': return self.calculate_EMeasure(y_pred, y_true)\n        elif metric == 'MAE': return self.calculate_MAE(y_pred, y_true)\n\n    def calculate_Accuracy(self, y_pred, y_true):\n        y_true = np.asarray(y_true.flatten(), dtype=np.int64)\n        y_pred = np.asarray(y_pred.flatten(), dtype=np.int64)\n\n        return accuracy_score(y_true, y_pred)\n\n    def calculate_Precision(self, y_pred, y_true):\n        y_true = np.asarray(y_true.flatten(), dtype=np.int64)\n        y_pred = np.asarray(y_pred.flatten(), dtype=np.int64)\n\n        return precision_score(y_true, y_pred)\n\n    def calculate_Recall(self, y_pred, y_true):\n        y_true = np.asarray(y_true.flatten(), dtype=np.int64)\n        y_pred = np.asarray(y_pred.flatten(), dtype=np.int64)\n\n        return recall_score(y_true, y_pred)\n\n    def calculate_Specificity(self, y_pred, y_true):\n        y_true = np.asarray(y_true.flatten(), dtype=np.int64)\n        y_pred = np.asarray(y_pred.flatten(), dtype=np.int64)\n\n        cm = list(confusion_matrix(y_true, y_pred).ravel())\n\n        if len(cm) == 1: cm += [0, 0, 0]\n\n        tn, fp, fn, tp = cm\n        specificity = tn / (tn+fp)\n\n        return specificity\n\n    def calculate_Jaccard(self, y_pred, y_true):\n        y_true = np.asarray(y_true.flatten(), dtype=np.int64)\n        y_pred = np.asarray(y_pred.flatten(), dtype=np.int64)\n\n        return jaccard_score(y_true, y_pred)\n\n    def calculate_DSC(self, y_pred, y_true):\n        y_true = np.asarray(y_true.flatten(), dtype=np.int64)\n        y_pred = np.asarray(y_pred.flatten(), dtype=np.int64)\n\n        intersection = np.sum(y_true * y_pred)\n        return (2. * intersection + self.smooth) / (np.sum(y_true) + np.sum(y_pred) + self.smooth)\n\n    def calculate_IoU(self, y_pred, y_true):\n        y_pred_f = y_pred > 0.5\n        y_true_f = y_true > 0.5\n\n        intersection_f = (y_pred_f & y_true_f).sum()\n        union_f = (y_pred_f | y_true_f).sum()\n\n        iou_f = (intersection_f + self.smooth) / (union_f + self.smooth)\n\n        return iou_f\n\n    def calculate_WeightedFMeasure(self, y_pred, y_true):\n        Dst, Idxt = bwdist(y_true == 0, return_indices=True)\n\n        E = np.abs(y_pred - y_true)\n        Et = np.copy(E)\n        Et[y_true == 0] = Et[Idxt[0][y_true == 0], Idxt[1][y_true == 0]]\n\n        K = self.matlab_style_gauss2D((7, 7), sigma=5)\n        EA = convolve(Et, weights=K, mode='constant', cval=0)\n        MIN_E_EA = np.where(np.array(y_true, dtype=np.bool_) & (EA < E), EA, E)\n\n        B = np.where(y_true == 0, 2 - np.exp(np.log(0.5) / 5 * Dst), np.ones_like(y_true))\n        Ew = MIN_E_EA * B\n\n        TPw = np.sum(y_true) - np.sum(Ew[y_true == 1])\n        FPw = np.sum(Ew[y_true == 0])\n\n        R = 1 - np.mean(Ew[np.array(y_true, dtype=np.bool_)])\n        P = TPw / (1e-6 + TPw + FPw)\n\n        Q = 2 * R * P / (1e-6 + R + P)\n\n        return Q\n\n    def calculate_FMeasure(self, y_pred, y_true):\n        th = 2 * y_pred.mean()\n        if th > 1:  th = 1\n        binary = np.zeros_like(y_pred)\n        binary[y_pred >= th] = 1\n        hard_gt = np.zeros_like(y_true)\n        hard_gt[y_true > 0.5] = 1\n        tp = (binary * hard_gt).sum()\n        if tp == 0:\n            meanF = 0\n        else:\n            pre = tp / binary.sum()\n            rec = tp / hard_gt.sum()\n            meanF = 1.3 * pre * rec / (0.3 * pre + rec)\n\n        return meanF\n\n    def calculate_SMeasure(self, y_pred, y_true):\n        y = np.mean(y_true)\n\n        if y == 0:\n            score = 1 - np.mean(y_pred)\n        elif y == 1:\n            score = np.mean(y_pred)\n        else:\n            score = 0.5 * self.object(y_pred, y_true) + 0.5 * self.region(y_pred, y_true)\n        return score\n\n    def calculate_EMeasure(self, y_pred, y_true):\n        th = 2 * y_pred.mean()\n        if th > 1:\n            th = 1\n        FM = np.zeros(y_true.shape)\n        FM[y_pred >= th] = 1\n        FM = np.array(FM,dtype=bool)\n        GT = np.array(y_true,dtype=bool)\n        dFM = np.double(FM)\n        if (sum(sum(np.double(GT)))==0):\n            enhanced_matrix = 1.0-dFM\n        elif (sum(sum(np.double(~GT)))==0):\n            enhanced_matrix = dFM\n        else:\n            dGT = np.double(GT)\n            align_matrix = self.AlignmentTerm(dFM, dGT)\n            enhanced_matrix = self.EnhancedAlignmentTerm(align_matrix)\n        [w, h] = np.shape(GT)\n        score = sum(sum(enhanced_matrix))/ (w * h - 1 + 1e-8)\n        return score\n\n    def calculate_MAE(self, y_pred, y_true):\n        return np.mean(np.abs(y_pred - y_true))\n\n    def matlab_style_gauss2D(self, shape=(7, 7), sigma=5):\n        \"\"\"\n        2D gaussian mask - should give the same result as MATLAB's\n        fspecial('gaussian',[shape],[sigma])\n        \"\"\"\n        m, n = [(ss - 1.) / 2. for ss in shape]\n        y, x = np.ogrid[-m:m + 1, -n:n + 1]\n        h = np.exp(-(x * x + y * y) / (2. * sigma * sigma))\n        h[h < np.finfo(h.dtype).eps * h.max()] = 0\n        sumh = h.sum()\n        if sumh != 0:\n            h /= sumh\n        return h\n\n    def object(self, pred, gt):\n        fg = pred * gt\n        bg = (1 - pred) * (1 - gt)\n\n        u = np.mean(gt)\n\n        return u * self.s_object(fg, gt) + (1 - u) * self.s_object(bg, np.logical_not(gt))\n\n    def s_object(self, in1, in2):\n        in1 = np.array(in1, dtype=np.int64)\n        in2 = np.array(in2, dtype=np.int64)\n\n        x = np.mean(in1[in2])\n        sigma_x = np.std(in1[in2])\n        return 2 * x / (pow(x, 2) + 1 + sigma_x + 1e-8)\n\n    def region(self, pred, gt):\n        [y, x] = ndimage.center_of_mass(gt)\n        y = int(round(y)) + 1\n        x = int(round(x)) + 1\n        [gt1, gt2, gt3, gt4, w1, w2, w3, w4] = self.divideGT(gt, x, y)\n        pred1, pred2, pred3, pred4 = self.dividePred(pred, x, y)\n\n        score1 = self.ssim(pred1, gt1)\n        score2 = self.ssim(pred2, gt2)\n        score3 = self.ssim(pred3, gt3)\n        score4 = self.ssim(pred4, gt4)\n\n        return w1 * score1 + w2 * score2 + w3 * score3 + w4 * score4\n\n    def divideGT(self, gt, x, y):\n        h, w = gt.shape\n        area = h * w\n        LT = gt[0:y, 0:x]\n        RT = gt[0:y, x:w]\n        LB = gt[y:h, 0:x]\n        RB = gt[y:h, x:w]\n\n        w1 = x * y / area\n        w2 = y * (w - x) / area\n        w3 = (h - y) * x / area\n        w4 = (h - y) * (w - x) / area\n\n        return LT, RT, LB, RB, w1, w2, w3, w4\n\n    def dividePred(self, pred, x, y):\n        h, w = pred.shape\n        LT = pred[0:y, 0:x]\n        RT = pred[0:y, x:w]\n        LB = pred[y:h, 0:x]\n        RB = pred[y:h, x:w]\n\n        return LT, RT, LB, RB\n\n    def ssim(self, in1, in2):\n        in2 = np.float32(in2)\n        h, w = in1.shape\n        N = h * w\n\n        x = np.mean(in1)\n        y = np.mean(in2)\n        sigma_x = np.var(in1)\n        sigma_y = np.var(in2)\n        sigma_xy = np.sum((in1 - x) * (in2 - y)) / (N - 1)\n\n        alpha = 4 * x * y * sigma_xy\n        beta = (x * x + y * y) * (sigma_x + sigma_y)\n\n        if alpha != 0:\n            score = alpha / (beta + 1e-8)\n        elif alpha == 0 and beta == 0:\n            score = 1\n        else:\n            score = 0\n\n        return score\n\n    def AlignmentTerm(self,dFM,dGT):\n        mu_FM = np.mean(dFM)\n        mu_GT = np.mean(dGT)\n        align_FM = dFM - mu_FM\n        align_GT = dGT - mu_GT\n        align_Matrix = 2. * (align_GT * align_FM)/ (align_GT* align_GT + align_FM* align_FM + 1e-8)\n        return align_Matrix\n\n    def EnhancedAlignmentTerm(self,align_Matrix):\n        enhanced = np.power(align_Matrix + 1,2) / 4\n        return enhanced\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.431729Z","iopub.execute_input":"2024-12-25T10:16:58.431965Z","iopub.status.idle":"2024-12-25T10:16:58.463558Z","shell.execute_reply.started":"2024-12-25T10:16:58.431944Z","shell.execute_reply":"2024-12-25T10:16:58.462705Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\nimport sys\n\n\nfrom torch.utils.data import DataLoader\n\n\nclass BaseSegmentationExperiment(object):\n    def __init__(self, args):\n        super(BaseSegmentationExperiment, self).__init__()\n\n        self.args = args\n\n        self.args.device = get_device()\n\n        \n        print(\"STEP1. Load {} Training Dataset Loader...\".format(args.train_data_type))\n        train_image_transform, train_target_transform = self.transform_generator()\n        if args.train_data_type == 'T1N':\n            train_dataset = Covid19CTScanDataset(args.train_dataset_dir, mode='train', transform=train_image_transform, target_transform=train_target_transform)\n            self.train_loader = DataLoader(train_dataset, batch_size=12, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n\n        print(\"STEP2. Load MADGNet ...\")\n        self.model = IS2D_model(args).to(self.args.device)\n\n        print(\"STEP2. Load MADGNet ...\")\n        self.model = IS2D_model(args)\n\n    def fix_seed(self):\n        random.seed(4321)\n        np.random.seed(4321)\n        torch.manual_seed(4321)\n        torch.cuda.manual_seed(4321)\n        torch.cuda.manual_seed_all(4321)\n\n    \n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.464939Z","iopub.execute_input":"2024-12-25T10:16:58.465169Z","iopub.status.idle":"2024-12-25T10:16:58.486144Z","shell.execute_reply.started":"2024-12-25T10:16:58.465150Z","shell.execute_reply":"2024-12-25T10:16:58.485325Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import os\nimport random\nimport sys\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nfrom PIL import Image\n\nclass Covid19CTScanDataset(Dataset) :\n    def __init__(self, dataset_dir, mode, transform=None, target_transform=None):\n        super(Covid19CTScanDataset, self).__init__()\n        self.image_folder = 'T1N'\n        self.label_folder = 'SEGMENTED'\n        self.mode = mode\n        self.mapping_dir = '/kaggle/input/mapping-files'\n        self.dataset_dir = '/kaggle/input/brats2021/datasetProcessed'\n        self.transform = transform\n        self.target_transform = target_transform\n        self.frame = pd.read_csv(os.path.join(self.mapping_dir, '{}_frame.csv'.format(mode)))#[:10]\n\n        print(len(self.frame))\n\n        if mode == 'train':\n            self.frame = pd.read_csv(os.path.join(self.mapping_dir, 'train_frame.csv'))\n            train, val = train_test_split(self.frame, test_size=0.2, shuffle=False, random_state=4321)\n\n            print(len(train))\n            print(len(val))\n\n            print(train)\n\n    def __len__(self):\n        return len(self.frame)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.dataset_dir, self.image_folder, self.frame.image_path[idx])\n        label_path = os.path.join(self.dataset_dir, self.label_folder, self.frame.mask_path[idx])\n\n        image = Image.open(image_path).convert('L')\n        label = Image.open(label_path).convert('L')\n\n        if self.transform:\n            seed = random.randint(0, 2 ** 32)\n            self._set_seed(seed); image = self.transform(image)\n            self._set_seed(seed); label = self.target_transform(label)\n\n        label[label >= 0.5] = 1; label[label < 0.5] = 0\n\n        return image, label\n\n    def _set_seed(self, seed):\n        random.seed(seed)\n        torch.manual_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.487472Z","iopub.execute_input":"2024-12-25T10:16:58.487701Z","iopub.status.idle":"2024-12-25T10:16:58.508320Z","shell.execute_reply.started":"2024-12-25T10:16:58.487683Z","shell.execute_reply":"2024-12-25T10:16:58.507469Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from tqdm import tqdm  \nimport torch\nimport random\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, jaccard_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.509066Z","iopub.execute_input":"2024-12-25T10:16:58.509265Z","iopub.status.idle":"2024-12-25T10:16:58.527755Z","shell.execute_reply.started":"2024-12-25T10:16:58.509249Z","shell.execute_reply":"2024-12-25T10:16:58.526893Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from types import SimpleNamespace\nimport torch\nimport torchvision.transforms as transforms\nimport numpy as np\n\ndef get_device():\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    print(\"You are using \\\"{}\\\" device.\".format(device))\n    return device\n\nclass BMISegmentationExperiment(object):\n    def __init__(self, args):\n        if isinstance(args, dict):\n            args = SimpleNamespace(**args)\n\n        super().__init__()\n\n        self.args = args  \n        self.args.device = get_device()  \n        \n        print(\"STEP1. Load {} Training Dataset Loader...\".format(args.train_data_type))\n        train_image_transform, train_target_transform = self.transform_generator()\n       \n        train_dataset = Covid19CTScanDataset(args.train_dataset_dir, mode='train', transform=train_image_transform, target_transform=train_target_transform)\n        self.train_loader = DataLoader(train_dataset, batch_size=12, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n\n        print(\"STEP2. Load MADGNet ...\")\n        self.model = IS2D_model(args).to(self.args.device)\n\n\n    def fix_seed(self):\n        random.seed(4321)\n        np.random.seed(4321)\n        torch.manual_seed(4321)\n        torch.cuda.manual_seed(4321)\n        torch.cuda.manual_seed_all(4321)\n\n    def forward(self, image, target, mode):\n        image, target = image.to(self.args.device), target.to(self.args.device)\n\n        with torch.cuda.amp.autocast(enabled=True):\n            output = self.model(image, mode)\n            loss = self.model._calculate_criterion(output, target)\n\n        return loss, output, target\n\n    def get_device():\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        print(\"You are using \\\"{}\\\" device.\".format(device))\n\n        return device\n\n    def transform_generator(self):\n        transform_list = [\n            transforms.Resize((self.args.image_size, self.args.image_size)),\n            transforms.ToTensor(),\n        ]\n\n        target_transform_list = [\n            transforms.Resize((self.args.image_size, self.args.image_size)),\n            transforms.ToTensor(),\n        ]\n\n        return transforms.Compose(transform_list), transforms.Compose(target_transform_list)\n\n    def validate(self, val_loader):\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for image, target in val_loader:\n                loss, _, _ = self.forward(image, target, mode='val')\n                total_loss += loss.item()\n\n        avg_loss = total_loss / len(val_loader)\n        return avg_loss\n\n    def train(self):\n        val_image_transform, val_target_transform = self.transform_generator()\n        val_dataset = Covid19CTScanDataset(self.args.train_dataset_dir, mode='val', transform=val_image_transform,\n                                           target_transform=val_target_transform)\n        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=self.args.num_workers,\n                                pin_memory=True)\n\n        best_val_loss = float('inf')\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n\n        for epoch in range(self.args.final_epoch):\n            self.model.train()\n            total_train_loss = 0\n\n            # Training loop\n            for image, target in self.train_loader:\n                loss, _, _ = self.forward(image, target, mode='train')\n                total_train_loss += loss.item()\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            avg_train_loss = total_train_loss / len(self.train_loader)\n\n            val_loss = self.validate(val_loader)\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(self.model.state_dict(), 'best_model.pth')\n\n            print(f\"Epoch [{epoch + 1}/{self.args.final_epoch}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n\n        print(\"Training completed. Saving the final model weights to 'final_model.pth'.\")\n        torch.save(self.model.state_dict(), 'final_model.pth')\n\n        print(\"Final model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.528679Z","iopub.execute_input":"2024-12-25T10:16:58.528905Z","iopub.status.idle":"2024-12-25T10:16:58.543078Z","shell.execute_reply.started":"2024-12-25T10:16:58.528886Z","shell.execute_reply":"2024-12-25T10:16:58.542310Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import os\nimport sys\n\nargs = {\n    \"num_workers\": 4,\n    \"data_path\": \"/kaggle/input/brats2021/datasetProcessed\",\n    \"train_data_type\": \"T1C\",\n    \"final_epoch\": 50,\n    \"metric_list\": ['DSC', 'IoU'],\n    \"metrics_list\": ['DSC', 'IoU', 'WeightedF-Measure', 'S-Measure', 'E-Measure', 'MAE'],\n    \"num_channels\": 1,\n    \"image_size\": 352,\n    \"in_channels\": 2048,\n    \"num_classes\": 1,\n    \"scale_branches\": 2,\n    \"frequency_branches\": 16,\n    \"frequency_selection\": 'top',\n    \"block_repetition\": 1,\n    \"min_channel\": 64,\n    \"min_resolution\": 8,\n    \"groups\": 32,\n    \"cnn_backbone\": 'resnest50'\n}\n\ndef IS2D_train(args):\n    try:\n        print(\"Started training for MRI Modality:\", args[\"train_data_type\"])\n        \n        if not args[\"data_path\"] or not args[\"train_data_type\"]:\n            raise ValueError(\"Both 'data_path' and 'train_d self.args.device = get_device(ata_type' must be defined in the arguments.\")\n        \n        args[\"train_dataset_dir\"] = os.path.join(args[\"data_path\"], args[\"train_data_type\"])\n        if not os.path.exists(args[\"train_dataset_dir\"]):\n            raise FileNotFoundError(f\"The dataset directory '{args['train_dataset_dir']}' does not exist.\")\n\n        experiment = BMISegmentationExperiment(args)\n        experiment.train()\n\n    except KeyError as e:\n        print(f\"KeyError: Missing key in arguments - {str(e)}\")\n        sys.exit(1)\n    except FileNotFoundError as e:\n        print(f\"FileNotFoundError: {str(e)}\")\n        sys.exit(1)\n    except ValueError as e:\n        print(f\"ValueError: {str(e)}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"An unexpected error occurred: {str(e)}\")\n        sys.exit(1)\n\nIS2D_train(args)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T10:16:58.544084Z","iopub.execute_input":"2024-12-25T10:16:58.544302Z","iopub.status.idle":"2024-12-25T11:07:59.874016Z","shell.execute_reply.started":"2024-12-25T10:16:58.544284Z","shell.execute_reply":"2024-12-25T11:07:59.872980Z"}},"outputs":[{"name":"stdout","text":"Started training for MRI Modality: T1C\nYou are using \"cuda\" device.\nSTEP1. Load T1C Training Dataset Loader...\n1038\n830\n208\n     Image_Id                     image_path                      mask_path\n0           0  BraTS-GLI-00005-100-t1n_1.jpg  BraTS-GLI-00005-100-seg_1.jpg\n1           1  BraTS-GLI-00005-100-t1n_2.jpg  BraTS-GLI-00005-100-seg_2.jpg\n2           2  BraTS-GLI-00005-100-t1n_3.jpg  BraTS-GLI-00005-100-seg_3.jpg\n3           3  BraTS-GLI-00005-101-t1n_1.jpg  BraTS-GLI-00005-101-seg_1.jpg\n4           4  BraTS-GLI-00005-101-t1n_2.jpg  BraTS-GLI-00005-101-seg_2.jpg\n..        ...                            ...                            ...\n825       825  BraTS-GLI-02826-100-t1n_1.jpg  BraTS-GLI-02826-100-seg_1.jpg\n826       826  BraTS-GLI-02826-100-t1n_2.jpg  BraTS-GLI-02826-100-seg_2.jpg\n827       827  BraTS-GLI-02826-100-t1n_3.jpg  BraTS-GLI-02826-100-seg_3.jpg\n828       828  BraTS-GLI-02826-101-t1n_1.jpg  BraTS-GLI-02826-101-seg_1.jpg\n829       829  BraTS-GLI-02826-101-t1n_2.jpg  BraTS-GLI-02826-101-seg_2.jpg\n\n[830 rows x 3 columns]\nSTEP2. Load MADGNet ...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-23-f912113e4217>:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/resnest/resnest50-528c19ca.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Complete loading your pretrained backbone resnest50\n44\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-29-7bb6cd2791e9>:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True):\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Train Loss: 1.1515, Validation Loss: 1.7503\nEpoch [2/50], Train Loss: 0.9767, Validation Loss: 1.7863\nEpoch [3/50], Train Loss: 0.9474, Validation Loss: 1.8846\nEpoch [4/50], Train Loss: 0.9306, Validation Loss: 1.4633\nEpoch [5/50], Train Loss: 0.9152, Validation Loss: 1.5874\nEpoch [6/50], Train Loss: 0.9104, Validation Loss: 1.4966\nEpoch [7/50], Train Loss: 0.9008, Validation Loss: 1.6459\nEpoch [8/50], Train Loss: 0.9042, Validation Loss: 1.6893\nEpoch [9/50], Train Loss: 0.8910, Validation Loss: 1.7420\nEpoch [10/50], Train Loss: 0.8856, Validation Loss: 1.6590\nEpoch [11/50], Train Loss: 0.8792, Validation Loss: 1.7112\nEpoch [12/50], Train Loss: 0.8796, Validation Loss: 1.6755\nEpoch [13/50], Train Loss: 0.8780, Validation Loss: 1.6906\nEpoch [14/50], Train Loss: 0.8856, Validation Loss: 1.7171\nEpoch [15/50], Train Loss: 0.8867, Validation Loss: 1.7520\nEpoch [16/50], Train Loss: 0.8697, Validation Loss: 1.6947\nEpoch [17/50], Train Loss: 0.8657, Validation Loss: 1.7312\nEpoch [18/50], Train Loss: 0.8560, Validation Loss: 1.5989\nEpoch [19/50], Train Loss: 0.8676, Validation Loss: 1.7706\nEpoch [20/50], Train Loss: 0.8603, Validation Loss: 1.2356\nEpoch [21/50], Train Loss: 0.8793, Validation Loss: 1.6148\nEpoch [22/50], Train Loss: 0.8698, Validation Loss: 1.7635\nEpoch [23/50], Train Loss: 0.8628, Validation Loss: 1.7180\nEpoch [24/50], Train Loss: 0.8566, Validation Loss: 2.2892\nEpoch [25/50], Train Loss: 0.8526, Validation Loss: 1.5322\nEpoch [26/50], Train Loss: 0.8523, Validation Loss: 1.2891\nEpoch [27/50], Train Loss: 0.8457, Validation Loss: 4.5511\nEpoch [28/50], Train Loss: 0.8475, Validation Loss: 3.0616\nEpoch [29/50], Train Loss: 0.8526, Validation Loss: 2.0204\nEpoch [30/50], Train Loss: 0.8469, Validation Loss: 1.7605\nEpoch [31/50], Train Loss: 0.8439, Validation Loss: 1.7288\nEpoch [32/50], Train Loss: 0.8529, Validation Loss: 1.5524\nEpoch [33/50], Train Loss: 1.0849, Validation Loss: 2.2531\nEpoch [34/50], Train Loss: 1.0837, Validation Loss: 1.7323\nEpoch [35/50], Train Loss: 1.0830, Validation Loss: 1.8474\nEpoch [36/50], Train Loss: 1.0807, Validation Loss: 1.7843\nEpoch [37/50], Train Loss: 1.0806, Validation Loss: 1.6651\nEpoch [38/50], Train Loss: 1.0823, Validation Loss: 1.6131\nEpoch [39/50], Train Loss: 1.0772, Validation Loss: 1.7202\nEpoch [40/50], Train Loss: 1.0778, Validation Loss: 1.7435\nEpoch [41/50], Train Loss: 1.0721, Validation Loss: 1.7449\nEpoch [42/50], Train Loss: 1.0758, Validation Loss: 1.7164\nEpoch [43/50], Train Loss: 1.0734, Validation Loss: 1.2144\nEpoch [44/50], Train Loss: 1.0733, Validation Loss: 1.7689\nEpoch [45/50], Train Loss: 1.0765, Validation Loss: 1.8276\nEpoch [46/50], Train Loss: 1.0742, Validation Loss: 1.7562\nEpoch [47/50], Train Loss: 1.0728, Validation Loss: 1.7931\nEpoch [48/50], Train Loss: 1.0700, Validation Loss: 1.2672\nEpoch [49/50], Train Loss: 1.0722, Validation Loss: 1.7412\nEpoch [50/50], Train Loss: 1.0718, Validation Loss: 1.8048\nTraining completed. Saving the final model weights to 'final_model.pth'.\nFinal model saved.\n","output_type":"stream"}],"execution_count":30}]}